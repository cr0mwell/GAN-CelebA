# GAN CelebA
DC and Wasserstein GANs trained on CelebA dataset using Keras library.

## Overview
Two modules contain models for DC GAN and Wasserstein GAN
that were written using [Keras library](https://keras.io/) and trained on [CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). It generates human faces.
Though the code was drastically overwritten, the baseline was taken from [another repository](https://github.com/YongWookHa/DCGAN-Keras)

## Required project folder structure
The following folders should be present in the project:
- _src/datasets/CelebA_ (folder where CelebA dataset should reside)
- _src/datasets/CelebA/<img_size>x<img_size>_ (folder with resized CelebA images)
- _src/models_ (folder that will contain saved trained models)
- _src/samples_ (folder with sampled images generated by the GAN during training)

## Usage
Install all third-party modules from requirements.txt file

You might need to resize the CelebA images. During my tests I was using 64x64 image size.
To do so you need to run crop_face() function that is present in base.py module.

You can then change the main constants:
- NUM_OF_EPOCHS;
- BATCH_SIZE;
- SAMPLE_INTERVAL - iterations period after which a new images samples will be generated and saved under 'src/samples/\<timestamp\>' directory;
- D_TRAIN_ITERATIONS(applicable for WGAN only) - number of training iterations for the discriminator in one training iteration of models;
- DEBUG - if set to 0 runs only 2 training epochs, otherwise if set to 1 runs NUM_OF_EPOCHS training epochs;

It's also possible to customize the GAN parameters during its initialization:
- _loss_: str (name of objective function), objective function or tf.keras.losses.Loss instance.
             See tf.keras.losses.
- _optimizer_: str (name of optimizer) or optimizer instance. See tf.keras.optimizers.
- _metrics_: list of metrics (or None) to be evaluated by the model during training and testing.
                Each of this can be a string (name of a built-in function), function
                or a tf.keras.metrics.Metric instance. See tf.keras.metrics.
- _dataset_: str. Name of the dataset that will be used to train the models.
- _img_size_: int. size of the images from the train dataset
- _channels_: int. Number of image channels from the train dataset
- _latent_dim_: int. Latent dimension size
- _generator_filter_: int. Filter size of generator's last layer
- _discriminator_filter_: int. Filter size of discriminator's first layer

Parameters used to continue training of previously trained models:
- _start_time_: str. Time stamp used by previously trained models.
- _last_epoch_: int. Last epoch when the model weights were saved to files.

Run the code to get some generated faces.

### Continue training of the pretrained model
It's possible to continue training the model if _'start_time'_ timestamp and _'last_epoch'_ arguments are provided during GAN initialization.
Example:
a model was previously trained and the following files with models weights were created under 'src/models/0821_2218':
- _discriminator_epoch2_weights.h5_
- _generator_epoch2_weights.h5_

You can continue to train the model by modifying the dcgan.py at line 151:

> gan = DCGAN(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'], start_time='0821_2218', last_epoch=2)
  

## Examples
- result after 60 epochs:

![Example](https://github.com/cr0mwell/GAN-CelebA/blob/master/example.png?raw=true)